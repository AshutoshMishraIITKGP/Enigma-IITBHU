{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf5af7f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-02T11:56:05.020468Z",
     "iopub.status.busy": "2026-02-02T11:56:05.020161Z",
     "iopub.status.idle": "2026-02-02T11:56:06.142965Z",
     "shell.execute_reply": "2026-02-02T11:56:06.141392Z"
    },
    "papermill": {
     "duration": 1.129599,
     "end_time": "2026-02-02T11:56:06.144845",
     "exception": false,
     "start_time": "2026-02-02T11:56:05.015246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/enigma26/Engima26_Dataset/test.xlsx\n",
      "/kaggle/input/enigma26/Engima26_Dataset/train.xlsx\n",
      "/kaggle/input/enigma26/Engima26_Dataset/target.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f02d852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:56:06.150916Z",
     "iopub.status.busy": "2026-02-02T11:56:06.150429Z",
     "iopub.status.idle": "2026-02-02T12:04:19.717263Z",
     "shell.execute_reply": "2026-02-02T12:04:19.715392Z"
    },
    "papermill": {
     "duration": 493.572526,
     "end_time": "2026-02-02T12:04:19.719649",
     "exception": false,
     "start_time": "2026-02-02T11:56:06.147123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üèÜ ENIGMA 2027 ‚Äî BUCKET + KNN HYBRID GENERATOR\n",
      "================================================================================\n",
      "Using dataset: /kaggle/input/enigma26/Engima26_Dataset\n",
      "Train: 600 | Test: 400 | Pairs: 360000\n",
      "\n",
      "Building feature memory...\n",
      "Building KDTree...\n",
      "Learning score buckets...\n",
      "\n",
      "Verifying TRAIN MSE...\n",
      "üèÜ TRAIN MSE: 0.0020865913598023043\n",
      "\n",
      "Generating submission...\n",
      "Progress: 50/400 | ETA: 122.3s\n",
      "Progress: 100/400 | ETA: 101.0s\n",
      "Progress: 150/400 | ETA: 85.1s\n",
      "Progress: 200/400 | ETA: 67.9s\n",
      "Progress: 250/400 | ETA: 49.9s\n",
      "Progress: 300/400 | ETA: 33.4s\n",
      "Progress: 350/400 | ETA: 16.6s\n",
      "Progress: 400/400 | ETA: 0.0s\n",
      "\n",
      "üèÜ DONE!\n",
      "File: submission_hybrid.csv\n",
      "Rows: 160000\n",
      "Time: 133.2 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1329"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üèÜ ENIGMA 2027 ‚Äî BUCKET + KNN HYBRID GENERATOR (FINAL CHAMPIONSHIP BUILD)\n",
    "# =============================================================================\n",
    "# GOAL:\n",
    "#   ‚Ä¢ Near-zero MSE on training\n",
    "#   ‚Ä¢ Strong private LB generalization\n",
    "#   ‚Ä¢ Reverse discrete generator behavior\n",
    "#\n",
    "# STRATEGY:\n",
    "#   1) Learn quantized score buckets from training data\n",
    "#   2) Memory-based KNN with distance weighting\n",
    "#   3) Hybrid confidence-based blending\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "import os, re, time, warnings, gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "DATA_DIR = \"/kaggle/input/enigma26/Engima26_Dataset\"\n",
    "\n",
    "KNN_K = 3\n",
    "FEATURE_WEIGHTS = np.array([1.0, 0.7, 0.7, 0.5])  # ALL, BI, BO, CO\n",
    "BUCKET_DECIMALS = 4   # j_all rounding for bucket key\n",
    "HYBRID_ALPHA = 0.7   # Bucket weight (0.7 bucket, 0.3 KNN)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üèÜ ENIGMA 2027 ‚Äî BUCKET + KNN HYBRID GENERATOR\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Using dataset:\", DATA_DIR)\n",
    "\n",
    "# =============================================================================\n",
    "# HELPERS\n",
    "# =============================================================================\n",
    "def normalize_token(x):\n",
    "    x = str(x).lower().strip()\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = re.sub(r\"[^\\w\\s]\", \"\", x)\n",
    "    return x.replace(\"&\", \"and\")\n",
    "\n",
    "def parse_set(val):\n",
    "    if pd.isna(val) or str(val).strip() in (\"\", \"nan\"):\n",
    "        return frozenset()\n",
    "    return frozenset(normalize_token(t) for t in str(val).split(\";\") if t.strip())\n",
    "\n",
    "def jaccard(a, b):\n",
    "    if not a and not b:\n",
    "        return 0.0\n",
    "    u = a | b\n",
    "    return len(a & b) / len(u) if u else 0.0\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n",
    "train_df = pd.read_excel(f\"{DATA_DIR}/train.xlsx\")\n",
    "test_df = pd.read_excel(f\"{DATA_DIR}/test.xlsx\")\n",
    "target_df = pd.read_csv(f\"{DATA_DIR}/target.csv\")\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Test: {len(test_df)} | Pairs: {len(target_df)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PREPROCESS\n",
    "# =============================================================================\n",
    "for df in [train_df, test_df]:\n",
    "    df[\"BI\"] = df[\"Business_Interests\"].apply(parse_set)\n",
    "    df[\"BO\"] = df[\"Business_Objectives\"].apply(parse_set)\n",
    "    df[\"CO\"] = df[\"Constraints\"].apply(parse_set)\n",
    "    df[\"ALL\"] = df.apply(lambda r: r[\"BI\"] | r[\"BO\"] | r[\"CO\"], axis=1)\n",
    "\n",
    "train_lookup = {r.Profile_ID: r for r in train_df.itertuples()}\n",
    "test_lookup = {r.Profile_ID: r for r in test_df.itertuples()}\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD TRAIN FEATURE MEMORY\n",
    "# =============================================================================\n",
    "print(\"\\nBuilding feature memory...\")\n",
    "\n",
    "pairs = target_df[target_df.src_user_id != target_df.dst_user_id]\n",
    "N = len(pairs)\n",
    "\n",
    "X = np.zeros((N, 4), dtype=np.float32)\n",
    "Y = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "for i, r in enumerate(pairs.itertuples()):\n",
    "    u1 = train_lookup[r.src_user_id]\n",
    "    u2 = train_lookup[r.dst_user_id]\n",
    "\n",
    "    f_all = jaccard(u1.ALL, u2.ALL)\n",
    "    f_bi = jaccard(u1.BI, u2.BI)\n",
    "    f_bo = jaccard(u1.BO, u2.BO)\n",
    "    f_co = jaccard(u1.CO, u2.CO)\n",
    "\n",
    "    X[i] = [f_all, f_bi, f_bo, f_co]\n",
    "    Y[i] = r.compatibility_score\n",
    "\n",
    "# =============================================================================\n",
    "# KNN MEMORY\n",
    "# =============================================================================\n",
    "XW = X * FEATURE_WEIGHTS\n",
    "print(\"Building KDTree...\")\n",
    "tree = KDTree(XW)\n",
    "\n",
    "# =============================================================================\n",
    "# BUCKET SYSTEM\n",
    "# =============================================================================\n",
    "print(\"Learning score buckets...\")\n",
    "\n",
    "bucket_map = {}\n",
    "for f, y in zip(X, Y):\n",
    "    key = round(f[0], BUCKET_DECIMALS)  # j_all bucket\n",
    "    if key not in bucket_map:\n",
    "        bucket_map[key] = []\n",
    "    bucket_map[key].append(y)\n",
    "\n",
    "for k in bucket_map:\n",
    "    bucket_map[k] = float(np.mean(bucket_map[k]))\n",
    "\n",
    "bucket_keys = np.array(sorted(bucket_map.keys()))\n",
    "BUCKET_EPS = np.std(Y) * 0.1  # Adaptive tolerance\n",
    "\n",
    "# =============================================================================\n",
    "# LOOKUPS\n",
    "# =============================================================================\n",
    "def bucket_lookup(j_all):\n",
    "    idx = np.argmin(np.abs(bucket_keys - j_all))\n",
    "    if abs(bucket_keys[idx] - j_all) <= BUCKET_EPS:\n",
    "        return bucket_map[bucket_keys[idx]], True\n",
    "    return None, False\n",
    "\n",
    "def knn_lookup(f_vec):\n",
    "    f = f_vec.reshape(1, -1) * FEATURE_WEIGHTS\n",
    "    dist, idx = tree.query(f, k=KNN_K)\n",
    "\n",
    "    d = dist[0] + 1e-6\n",
    "    w = 1 / d\n",
    "    return float(np.sum(w * Y[idx[0]]) / np.sum(w))\n",
    "\n",
    "# =============================================================================\n",
    "# HYBRID GENERATOR\n",
    "# =============================================================================\n",
    "def hybrid_generator(u1, u2):\n",
    "    f_all = jaccard(u1.ALL, u2.ALL)\n",
    "    f_bi = jaccard(u1.BI, u2.BI)\n",
    "    f_bo = jaccard(u1.BO, u2.BO)\n",
    "    f_co = jaccard(u1.CO, u2.CO)\n",
    "\n",
    "    features = np.array([f_all, f_bi, f_bo, f_co])\n",
    "\n",
    "    bucket_score, ok = bucket_lookup(f_all)\n",
    "    knn_score = knn_lookup(features)\n",
    "\n",
    "    if ok:\n",
    "        return HYBRID_ALPHA * bucket_score + (1 - HYBRID_ALPHA) * knn_score\n",
    "    return knn_score\n",
    "\n",
    "# =============================================================================\n",
    "# SELF SCORE\n",
    "# =============================================================================\n",
    "self_pairs = target_df[target_df.src_user_id == target_df.dst_user_id]\n",
    "SELF_SCORE = float(self_pairs.compatibility_score.iloc[0]) if len(self_pairs) else 0.0\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN MSE CHECK\n",
    "# =============================================================================\n",
    "print(\"\\nVerifying TRAIN MSE...\")\n",
    "\n",
    "train_preds = []\n",
    "train_true = []\n",
    "\n",
    "for r in pairs.itertuples():\n",
    "    u1 = train_lookup[r.src_user_id]\n",
    "    u2 = train_lookup[r.dst_user_id]\n",
    "    train_preds.append(hybrid_generator(u1, u2))\n",
    "    train_true.append(r.compatibility_score)\n",
    "\n",
    "train_mse = np.mean((np.array(train_preds) - np.array(train_true)) ** 2)\n",
    "print(\"üèÜ TRAIN MSE:\", train_mse)\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE SUBMISSION\n",
    "# =============================================================================\n",
    "print(\"\\nGenerating submission...\")\n",
    "\n",
    "ids = sorted(test_df.Profile_ID.unique())\n",
    "results = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, src in enumerate(ids):\n",
    "    u1 = test_lookup[src]\n",
    "    for dst in ids:\n",
    "        if src == dst:\n",
    "            score = SELF_SCORE\n",
    "        else:\n",
    "            u2 = test_lookup[dst]\n",
    "            score = hybrid_generator(u1, u2)\n",
    "\n",
    "        results.append({\n",
    "            \"ID\": f\"{src}_{dst}\",\n",
    "            \"compatibility_score\": float(score)\n",
    "        })\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        elapsed = time.time() - start\n",
    "        eta = elapsed / (i + 1) * (len(ids) - i - 1)\n",
    "        print(f\"Progress: {i+1}/{len(ids)} | ETA: {eta:.1f}s\")\n",
    "\n",
    "submission = pd.DataFrame(results)\n",
    "submission.to_csv(\"submission_hybrid.csv\", index=False)\n",
    "\n",
    "print(\"\\nüèÜ DONE!\")\n",
    "print(\"File: submission_hybrid.csv\")\n",
    "print(\"Rows:\", len(submission))\n",
    "print(\"Time:\", round(time.time() - start, 1), \"seconds\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e462fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T12:04:19.726514Z",
     "iopub.status.busy": "2026-02-02T12:04:19.726225Z",
     "iopub.status.idle": "2026-02-02T13:50:44.903452Z",
     "shell.execute_reply": "2026-02-02T13:50:44.902084Z"
    },
    "papermill": {
     "duration": 6385.182944,
     "end_time": "2026-02-02T13:50:44.905294",
     "exception": false,
     "start_time": "2026-02-02T12:04:19.722350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üî• ENIGMA 2027 - ULTRA CHAMPIONSHIP SOLUTION v3.0 üî•\n",
      "======================================================================\n",
      "\n",
      "Models available: {'lgb': True, 'xgb': True, 'catboost': True}\n",
      "\n",
      "[1] Loading data...\n",
      "  ‚Üí Using: /kaggle/input/enigma26/Engima26_Dataset\n",
      "  Train: 600 users | Test: 400 users | Pairs: 360000\n",
      "\n",
      "[2] Building domain knowledge...\n",
      "  ‚úì Enhanced role synergy & industry clusters\n",
      "\n",
      "[3] Ultra text normalization...\n",
      "  ‚úì Parsed 600 train + 400 test profiles\n",
      "\n",
      "[4] Defining similarity functions...\n",
      "  ‚úì Jaccard, Dice, Overlap coefficient\n",
      "\n",
      "[5] Discovering optimal formula...\n",
      "  Self-pairs: 600, score=0.0\n",
      "    Union Jaccard: MSE=0.0025631469\n",
      "\n",
      "  ‚òÖ WINNER: Union Jaccard (MSE=0.002563146948)\n",
      "\n",
      "[6] Ultra feature engineering (50+ features)...\n",
      "  Building training features...\n",
      "  ‚úì Features: 57 | Samples: 359400\n",
      "\n",
      "[7] Training multi-model ensemble with stacking...\n",
      "  Training LightGBM...\n",
      "    LightGBM OOF MSE: 0.0004368912\n",
      "  Training XGBoost...\n",
      "    XGBoost OOF MSE: 0.0003885447\n",
      "  Training CatBoost...\n",
      "    CatBoost OOF MSE: 0.0004403534\n",
      "  Training GradientBoosting...\n",
      "    GradientBoosting OOF MSE: 0.0004396896\n",
      "  Training Ridge...\n",
      "    Ridge OOF MSE: 0.0005260216\n",
      "  Training MLP...\n",
      "    MLP OOF MSE: 0.0004757638\n",
      "  Training ExtraTrees...\n",
      "    ExtraTrees OOF MSE: 0.0004558661\n",
      "\n",
      "[8] Training meta-learner for stacking...\n",
      "  Stacking features shape: (359400, 7)\n",
      "  Meta-learner OOF MSE: 0.0003685904\n",
      "\n",
      "  Finding optimal ensemble weights...\n",
      "  Simple average MSE: 0.0004380091\n",
      "  Inverse-MSE weighted MSE: 0.0004353987\n",
      "\n",
      "  ‚òÖ Using META-LEARNER (MSE=0.0003685904)\n",
      "\n",
      "  Model weights: {'lgb': 0.14668280915879786, 'xgb': 0.16493453025330565, 'catboost': 0.14552956341793505, 'gb': 0.1457492540429237, 'ridge': 0.12182853041715128, 'mlp': 0.13469801115407823, 'et': 0.1405773015558082}\n",
      "\n",
      "[9] Generating test predictions...\n",
      "  Test users: 400 | Pairs: 160000\n",
      "  Processing 1/400...\n",
      "  Processing 101/400...\n",
      "  Processing 201/400...\n",
      "  Processing 301/400...\n",
      "  Computing model predictions...\n",
      "\n",
      "[10] Creating submission...\n",
      "  ‚úì Shape: (160000, 2)\n",
      "  ‚úì Score range: [0.0000, 1.0000]\n",
      "  ‚úì Self-pairs: 400, score=1.0\n",
      "\n",
      "  Sample:\n",
      "       ID  compatibility_score\n",
      "5601_5601               1.0000\n",
      "5601_5602               0.1560\n",
      "5601_5603               0.0242\n",
      "5601_5604               0.1943\n",
      "5601_5605               0.1007\n",
      "5601_5606               0.0838\n",
      "5601_5607               0.0048\n",
      "5601_5608               0.1752\n",
      "5601_5609               0.0073\n",
      "5601_5610               0.1865\n",
      "\n",
      "  ‚úì Saved: submission.csv\n",
      "\n",
      "======================================================================\n",
      "üî• ULTRA CHAMPIONSHIP SOLUTION COMPLETE! üî•\n",
      "======================================================================\n",
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë  ULTRA WINNING CONFIGURATION                                         ‚ïë\n",
      "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
      "‚ïë  ‚Ä¢ Models: 7 (LGB/XGB/CAT/GB/Ridge/MLP/ET)                      ‚ïë\n",
      "‚ïë  ‚Ä¢ Features: 57                                              ‚ïë\n",
      "‚ïë  ‚Ä¢ Stacking: META-LEARNER                             ‚ïë\n",
      "‚ïë  ‚Ä¢ Final Train MSE: 0.0003685904                             ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "MODEL PERFORMANCE:\n",
      "\n",
      "   ‚Ä¢ lgb         : MSE=0.0004368912\n",
      "   ‚Ä¢ xgb         : MSE=0.0003885447\n",
      "   ‚Ä¢ catboost    : MSE=0.0004403534\n",
      "   ‚Ä¢ gb          : MSE=0.0004396896\n",
      "   ‚Ä¢ ridge       : MSE=0.0005260216\n",
      "   ‚Ä¢ mlp         : MSE=0.0004757638\n",
      "   ‚Ä¢ et          : MSE=0.0004558661\n",
      "\n",
      "ENSEMBLE WEIGHTS:\n",
      "\n",
      "   ‚Ä¢ lgb         : 0.1467\n",
      "   ‚Ä¢ xgb         : 0.1649\n",
      "   ‚Ä¢ catboost    : 0.1455\n",
      "   ‚Ä¢ gb          : 0.1457\n",
      "   ‚Ä¢ ridge       : 0.1218\n",
      "   ‚Ä¢ mlp         : 0.1347\n",
      "   ‚Ä¢ et          : 0.1406\n",
      "\n",
      "OUTPUT: submission.csv (160,000 rows)\n",
      "\n",
      "üèÜ Good luck! This is maximum optimization! üèÜ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "======================================================================================\n",
    "ENIGMA 2027 - ULTRA CHAMPIONSHIP SOLUTION v3.0\n",
    "======================================================================================\n",
    "üî• MAXIMUM OPTIMIZATION FOR LOWEST POSSIBLE MSE üî•\n",
    "\n",
    "UPGRADES FROM v2.0:\n",
    "  1. Multi-Model Ensemble (LightGBM + XGBoost + CatBoost + Ridge + MLP)\n",
    "  2. Stacking with Meta-Learner\n",
    "  3. 50+ Enhanced Features\n",
    "  4. Hyperparameter Optimization\n",
    "  5. Advanced Feature Engineering\n",
    "  6. K-Fold Stacking for Robustness\n",
    "\n",
    "======================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import all available ML libraries\n",
    "MODELS_AVAILABLE = {}\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    MODELS_AVAILABLE['lgb'] = True\n",
    "except:\n",
    "    MODELS_AVAILABLE['lgb'] = False\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    MODELS_AVAILABLE['xgb'] = True\n",
    "except:\n",
    "    MODELS_AVAILABLE['xgb'] = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    MODELS_AVAILABLE['catboost'] = True\n",
    "except:\n",
    "    MODELS_AVAILABLE['catboost'] = False\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üî• ENIGMA 2027 - ULTRA CHAMPIONSHIP SOLUTION v3.0 üî•\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModels available: {MODELS_AVAILABLE}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "print(\"\\n[1] Loading data...\")\n",
    "\n",
    "DATA_DIR = '/kaggle/input/enigma26/Engima26_Dataset'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    DATA_DIR = '.'\n",
    "print(f\"  ‚Üí Using: {DATA_DIR}\")\n",
    "\n",
    "train_df = pd.read_excel(f'{DATA_DIR}/train.xlsx')\n",
    "test_df = pd.read_excel(f'{DATA_DIR}/test.xlsx')\n",
    "target_df = pd.read_csv(f'{DATA_DIR}/target.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} users | Test: {len(test_df)} users | Pairs: {len(target_df)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: DOMAIN KNOWLEDGE MATRICES\n",
    "# =============================================================================\n",
    "print(\"\\n[2] Building domain knowledge...\")\n",
    "\n",
    "ROLE_SYNERGY = {\n",
    "    ('founder', 'investor'): 1.0, ('founder', 'mentor'): 0.95, ('founder', 'advisor'): 0.9,\n",
    "    ('founder', 'engineer'): 0.8, ('founder', 'developer'): 0.8, ('investor', 'ceo'): 0.95,\n",
    "    ('engineer', 'manager'): 0.85, ('developer', 'manager'): 0.85, ('cto', 'engineer'): 0.9,\n",
    "    ('cto', 'developer'): 0.9, ('ceo', 'cto'): 0.9, ('ceo', 'cfo'): 0.85,\n",
    "    ('sales', 'marketing'): 0.9, ('sales', 'product'): 0.8, ('marketing', 'product'): 0.85,\n",
    "    ('consultant', 'executive'): 0.85, ('consultant', 'manager'): 0.8,\n",
    "    ('analyst', 'manager'): 0.8, ('analyst', 'consultant'): 0.85,\n",
    "    ('data', 'engineer'): 0.9, ('data', 'analyst'): 0.9,\n",
    "    ('hr', 'manager'): 0.8, ('hr', 'executive'): 0.75,\n",
    "}\n",
    "\n",
    "INDUSTRY_CLUSTERS = {\n",
    "    'tech': ['technology', 'software', 'saas', 'ai', 'fintech', 'edtech', 'healthtech', 'tech', 'it'],\n",
    "    'finance': ['finance', 'banking', 'investment', 'fintech', 'insurance', 'financial'],\n",
    "    'healthcare': ['healthcare', 'medical', 'biotech', 'pharma', 'healthtech', 'health'],\n",
    "    'media': ['media', 'entertainment', 'content', 'gaming', 'digital'],\n",
    "    'retail': ['retail', 'e-commerce', 'ecommerce', 'consumer', 'cpg'],\n",
    "    'manufacturing': ['manufacturing', 'industrial', 'supply chain', 'logistics'],\n",
    "}\n",
    "\n",
    "def role_score(r1, r2):\n",
    "    if not r1 or not r2 or pd.isna(r1) or pd.isna(r2): return 0.5\n",
    "    r1, r2 = str(r1).lower().strip(), str(r2).lower().strip()\n",
    "    if r1 == r2: return 0.7\n",
    "    for (a, b), s in ROLE_SYNERGY.items():\n",
    "        if (a in r1 and b in r2) or (b in r1 and a in r2): return s\n",
    "    return 0.4\n",
    "\n",
    "def industry_score(i1, i2):\n",
    "    if not i1 or not i2 or pd.isna(i1) or pd.isna(i2): return 0.5\n",
    "    i1, i2 = str(i1).lower(), str(i2).lower()\n",
    "    if i1 == i2: return 1.0\n",
    "    c1 = c2 = None\n",
    "    for c, lst in INDUSTRY_CLUSTERS.items():\n",
    "        if any(x in i1 for x in lst): c1 = c\n",
    "        if any(x in i2 for x in lst): c2 = c\n",
    "    if c1 and c2: return 0.85 if c1 == c2 else 0.35\n",
    "    return 0.3\n",
    "\n",
    "print(\"  ‚úì Enhanced role synergy & industry clusters\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: PREPROCESSING (ULTRA TEXT NORMALIZATION)\n",
    "# =============================================================================\n",
    "print(\"\\n[3] Ultra text normalization...\")\n",
    "\n",
    "def normalize_token(x):\n",
    "    x = x.lower().strip()\n",
    "    x = re.sub(r'\\s+', ' ', x)\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)  # Remove punctuation\n",
    "    # Comprehensive synonyms\n",
    "    synonyms = {\n",
    "        'artificial intelligence': 'ai', 'machine learning': 'ml',\n",
    "        'deep learning': 'dl', 'natural language processing': 'nlp',\n",
    "        'computer vision': 'cv', 'data science': 'ds',\n",
    "        'software as a service': 'saas', 'platform as a service': 'paas',\n",
    "        'infrastructure as a service': 'iaas',\n",
    "        'business to business': 'b2b', 'business to consumer': 'b2c',\n",
    "        'research and development': 'rd', 'mergers and acquisitions': 'ma',\n",
    "        'chief executive officer': 'ceo', 'chief technology officer': 'cto',\n",
    "        'chief financial officer': 'cfo', 'chief operating officer': 'coo',\n",
    "        'venture capital': 'vc', 'private equity': 'pe',\n",
    "        'initial public offering': 'ipo',\n",
    "    }\n",
    "    for k, v in synonyms.items():\n",
    "        x = x.replace(k, v)\n",
    "    x = x.replace('&', 'and')\n",
    "    return x\n",
    "\n",
    "def parse_set(val):\n",
    "    if pd.isna(val) or str(val) == 'nan': return set()\n",
    "    return {normalize_token(x) for x in str(val).split(';') if x.strip() and x.strip() != 'nan'}\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df['BI'] = df['Business_Interests'].apply(parse_set)\n",
    "    df['BO'] = df['Business_Objectives'].apply(parse_set)\n",
    "    df['CO'] = df['Constraints'].apply(parse_set)\n",
    "    df['ALL'] = df.apply(lambda r: r['BI'] | r['BO'] | r['CO'], axis=1)\n",
    "    df['BI_BO'] = df.apply(lambda r: r['BI'] | r['BO'], axis=1)\n",
    "    return df\n",
    "\n",
    "train_p = preprocess(train_df)\n",
    "test_p = preprocess(test_df)\n",
    "print(f\"  ‚úì Parsed {len(train_p)} train + {len(test_p)} test profiles\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: SIMILARITY FUNCTIONS\n",
    "# =============================================================================\n",
    "print(\"\\n[4] Defining similarity functions...\")\n",
    "\n",
    "def jaccard(s1, s2):\n",
    "    if not s1 and not s2: return 0.0\n",
    "    return len(s1 & s2) / len(s1 | s2) if len(s1 | s2) > 0 else 0.0\n",
    "\n",
    "def dice(s1, s2):\n",
    "    if not s1 and not s2: return 0.0\n",
    "    return 2 * len(s1 & s2) / (len(s1) + len(s2)) if (len(s1) + len(s2)) > 0 else 0.0\n",
    "\n",
    "def overlap_coef(s1, s2):\n",
    "    if not s1 or not s2: return 0.0\n",
    "    return len(s1 & s2) / min(len(s1), len(s2)) if min(len(s1), len(s2)) > 0 else 0.0\n",
    "\n",
    "def union_jaccard(r1, r2):\n",
    "    return jaccard(r1['ALL'], r2['ALL'])\n",
    "\n",
    "print(\"  ‚úì Jaccard, Dice, Overlap coefficient\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: FORMULA DISCOVERY\n",
    "# =============================================================================\n",
    "print(\"\\n[5] Discovering optimal formula...\")\n",
    "\n",
    "self_pairs = target_df[target_df['src_user_id'] == target_df['dst_user_id']]\n",
    "SELF_SCORE = self_pairs['compatibility_score'].iloc[0] if len(self_pairs) > 0 else 1.0\n",
    "print(f\"  Self-pairs: {len(self_pairs)}, score={SELF_SCORE}\")\n",
    "\n",
    "train_lookup = {r['Profile_ID']: r for _, r in train_p.iterrows()}\n",
    "\n",
    "def test_formula(func):\n",
    "    preds, acts = [], []\n",
    "    for row in target_df.itertuples():\n",
    "        if row.src_user_id == row.dst_user_id: continue\n",
    "        preds.append(func(train_lookup[row.src_user_id], train_lookup[row.dst_user_id]))\n",
    "        acts.append(row.compatibility_score)\n",
    "    return mean_squared_error(acts, preds), np.array(preds), np.array(acts)\n",
    "\n",
    "best_mse, best_func, best_name = float('inf'), None, \"\"\n",
    "formulas = [(\"Union Jaccard\", union_jaccard)]\n",
    "for name, func in formulas:\n",
    "    mse, preds, acts = test_formula(func)\n",
    "    print(f\"    {name}: MSE={mse:.10f}\")\n",
    "    if mse < best_mse:\n",
    "        best_mse, best_func, best_name = mse, func, name\n",
    "        formula_preds, y_train = preds, acts\n",
    "\n",
    "print(f\"\\n  ‚òÖ WINNER: {best_name} (MSE={best_mse:.12f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: ULTRA FEATURE ENGINEERING (50+ Features)\n",
    "# =============================================================================\n",
    "print(\"\\n[6] Ultra feature engineering (50+ features)...\")\n",
    "\n",
    "def extract_ultra_features(r1, r2):\n",
    "    \"\"\"Extract comprehensive feature set\"\"\"\n",
    "    f = {}\n",
    "    \n",
    "    # === Jaccard-based features ===\n",
    "    f['j_all'] = jaccard(r1['ALL'], r2['ALL'])\n",
    "    f['j_bi'] = jaccard(r1['BI'], r2['BI'])\n",
    "    f['j_bo'] = jaccard(r1['BO'], r2['BO'])\n",
    "    f['j_co'] = jaccard(r1['CO'], r2['CO'])\n",
    "    f['j_bi_bo'] = jaccard(r1['BI_BO'], r2['BI_BO'])\n",
    "    \n",
    "    # === Dice coefficient ===\n",
    "    f['dice_all'] = dice(r1['ALL'], r2['ALL'])\n",
    "    f['dice_bi'] = dice(r1['BI'], r2['BI'])\n",
    "    f['dice_bo'] = dice(r1['BO'], r2['BO'])\n",
    "    \n",
    "    # === Overlap coefficient ===\n",
    "    f['overlap_all'] = overlap_coef(r1['ALL'], r2['ALL'])\n",
    "    f['overlap_bi'] = overlap_coef(r1['BI'], r2['BI'])\n",
    "    \n",
    "    # === Set size features ===\n",
    "    f['bi_size_1'] = len(r1['BI'])\n",
    "    f['bi_size_2'] = len(r2['BI'])\n",
    "    f['bo_size_1'] = len(r1['BO'])\n",
    "    f['bo_size_2'] = len(r2['BO'])\n",
    "    f['co_size_1'] = len(r1['CO'])\n",
    "    f['co_size_2'] = len(r2['CO'])\n",
    "    f['all_size_1'] = len(r1['ALL'])\n",
    "    f['all_size_2'] = len(r2['ALL'])\n",
    "    \n",
    "    # === Interaction features ===\n",
    "    f['bi_inter'] = len(r1['BI'] & r2['BI'])\n",
    "    f['bo_inter'] = len(r1['BO'] & r2['BO'])\n",
    "    f['co_inter'] = len(r1['CO'] & r2['CO'])\n",
    "    f['all_inter'] = len(r1['ALL'] & r2['ALL'])\n",
    "    \n",
    "    f['bi_union'] = len(r1['BI'] | r2['BI'])\n",
    "    f['bo_union'] = len(r1['BO'] | r2['BO'])\n",
    "    f['co_union'] = len(r1['CO'] | r2['CO'])\n",
    "    f['all_union'] = len(r1['ALL'] | r2['ALL'])\n",
    "    \n",
    "    # === Ratio features ===\n",
    "    f['overlap_ratio_1'] = len(r1['ALL'] & r2['ALL']) / (len(r1['ALL']) + 1e-6)\n",
    "    f['overlap_ratio_2'] = len(r1['ALL'] & r2['ALL']) / (len(r2['ALL']) + 1e-6)\n",
    "    f['size_ratio'] = min(len(r1['ALL']), len(r2['ALL'])) / (max(len(r1['ALL']), len(r2['ALL'])) + 1e-6)\n",
    "    f['size_diff'] = abs(len(r1['ALL']) - len(r2['ALL']))\n",
    "    f['size_sum'] = len(r1['ALL']) + len(r2['ALL'])\n",
    "    f['size_product'] = len(r1['ALL']) * len(r2['ALL'])\n",
    "    \n",
    "    # === Cross-category features ===\n",
    "    f['bi_bo_cross'] = len(r1['BI'] & r2['BO']) + len(r1['BO'] & r2['BI'])\n",
    "    f['bi_co_cross'] = len(r1['BI'] & r2['CO']) + len(r1['CO'] & r2['BI'])\n",
    "    f['bo_co_cross'] = len(r1['BO'] & r2['CO']) + len(r1['CO'] & r2['BO'])\n",
    "    \n",
    "    # === Asymmetry features ===\n",
    "    f['asymmetry_bi'] = abs(len(r1['BI']) - len(r2['BI'])) / (len(r1['BI']) + len(r2['BI']) + 1e-6)\n",
    "    f['asymmetry_all'] = abs(len(r1['ALL']) - len(r2['ALL'])) / (len(r1['ALL']) + len(r2['ALL']) + 1e-6)\n",
    "    \n",
    "    # === Demographic features ===\n",
    "    if 'Age' in r1 and 'Age' in r2:\n",
    "        a1 = r1.get('Age', 30) if pd.notna(r1.get('Age')) else 30\n",
    "        a2 = r2.get('Age', 30) if pd.notna(r2.get('Age')) else 30\n",
    "        f['age_diff'] = abs(a1 - a2)\n",
    "        f['age_sum'] = a1 + a2\n",
    "        f['age_min'] = min(a1, a2)\n",
    "        f['age_max'] = max(a1, a2)\n",
    "        f['age_ratio'] = min(a1, a2) / (max(a1, a2) + 1e-6)\n",
    "    \n",
    "    if 'Role' in r1 and 'Role' in r2:\n",
    "        f['role_synergy'] = role_score(r1.get('Role'), r2.get('Role'))\n",
    "        f['same_role'] = 1 if str(r1.get('Role', '')).lower() == str(r2.get('Role', '')).lower() else 0\n",
    "    \n",
    "    if 'Industry' in r1 and 'Industry' in r2:\n",
    "        f['industry_align'] = industry_score(r1.get('Industry'), r2.get('Industry'))\n",
    "        f['same_industry'] = 1 if str(r1.get('Industry', '')).lower() == str(r2.get('Industry', '')).lower() else 0\n",
    "    \n",
    "    if 'Location_City' in r1 and 'Location_City' in r2:\n",
    "        f['same_city'] = 1 if str(r1.get('Location_City', '')).lower() == str(r2.get('Location_City', '')).lower() else 0\n",
    "    \n",
    "    if 'Seniority_Level' in r1 and 'Seniority_Level' in r2:\n",
    "        f['same_seniority'] = 1 if str(r1.get('Seniority_Level', '')).lower() == str(r2.get('Seniority_Level', '')).lower() else 0\n",
    "        seniority_map = {'junior': 1, 'mid': 2, 'senior': 3, 'executive': 4}\n",
    "        s1 = seniority_map.get(str(r1.get('Seniority_Level', '')).lower(), 2)\n",
    "        s2 = seniority_map.get(str(r2.get('Seniority_Level', '')).lower(), 2)\n",
    "        f['seniority_diff'] = abs(s1 - s2)\n",
    "    \n",
    "    if 'Gender' in r1 and 'Gender' in r2:\n",
    "        f['same_gender'] = 1 if str(r1.get('Gender', '')).lower() == str(r2.get('Gender', '')).lower() else 0\n",
    "    \n",
    "    if 'Company_Size_Employees' in r1 and 'Company_Size_Employees' in r2:\n",
    "        cs1 = r1.get('Company_Size_Employees', 100) if pd.notna(r1.get('Company_Size_Employees')) else 100\n",
    "        cs2 = r2.get('Company_Size_Employees', 100) if pd.notna(r2.get('Company_Size_Employees')) else 100\n",
    "        f['company_size_ratio'] = min(cs1, cs2) / (max(cs1, cs2) + 1e-6)\n",
    "        f['company_size_diff'] = abs(cs1 - cs2)\n",
    "    \n",
    "    # === Polynomial features (key interactions) ===\n",
    "    f['j_all_sq'] = f['j_all'] ** 2\n",
    "    f['j_bi_sq'] = f['j_bi'] ** 2\n",
    "    f['j_all_j_bi'] = f['j_all'] * f['j_bi']\n",
    "    f['j_all_j_bo'] = f['j_all'] * f['j_bo']\n",
    "    f['j_bi_j_bo'] = f['j_bi'] * f['j_bo']\n",
    "    \n",
    "    return f\n",
    "\n",
    "# Build training data\n",
    "print(\"  Building training features...\")\n",
    "X_list, y_list = [], []\n",
    "for row in target_df.itertuples():\n",
    "    if row.src_user_id == row.dst_user_id: continue\n",
    "    X_list.append(extract_ultra_features(train_lookup[row.src_user_id], train_lookup[row.dst_user_id]))\n",
    "    y_list.append(row.compatibility_score)\n",
    "\n",
    "X_train = pd.DataFrame(X_list)\n",
    "y_train = np.array(y_list)\n",
    "print(f\"  ‚úì Features: {X_train.shape[1]} | Samples: {len(y_train)}\")\n",
    "\n",
    "# Scale features for neural network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: MULTI-MODEL TRAINING WITH STACKING\n",
    "# =============================================================================\n",
    "print(\"\\n[7] Training multi-model ensemble with stacking...\")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_predictions = {}\n",
    "models = {}\n",
    "\n",
    "# --- Model 1: LightGBM ---\n",
    "if MODELS_AVAILABLE.get('lgb', False):\n",
    "    print(\"  Training LightGBM...\")\n",
    "    lgb_params = {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'min_child_samples': 20,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    oof_lgb = cross_val_predict(lgb_model, X_train, y_train, cv=kf)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_mse = mean_squared_error(y_train, oof_lgb)\n",
    "    print(f\"    LightGBM OOF MSE: {lgb_mse:.10f}\")\n",
    "    oof_predictions['lgb'] = oof_lgb\n",
    "    models['lgb'] = lgb_model\n",
    "\n",
    "# --- Model 2: XGBoost ---\n",
    "if MODELS_AVAILABLE.get('xgb', False):\n",
    "    print(\"  Training XGBoost...\")\n",
    "    xgb_params = {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    oof_xgb = cross_val_predict(xgb_model, X_train, y_train, cv=kf)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_mse = mean_squared_error(y_train, oof_xgb)\n",
    "    print(f\"    XGBoost OOF MSE: {xgb_mse:.10f}\")\n",
    "    oof_predictions['xgb'] = oof_xgb\n",
    "    models['xgb'] = xgb_model\n",
    "\n",
    "# --- Model 3: CatBoost ---\n",
    "if MODELS_AVAILABLE.get('catboost', False):\n",
    "    print(\"  Training CatBoost...\")\n",
    "    cat_model = CatBoostRegressor(\n",
    "        iterations=500,\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    oof_cat = cross_val_predict(cat_model, X_train, y_train, cv=kf)\n",
    "    cat_model.fit(X_train, y_train)\n",
    "    cat_mse = mean_squared_error(y_train, oof_cat)\n",
    "    print(f\"    CatBoost OOF MSE: {cat_mse:.10f}\")\n",
    "    oof_predictions['catboost'] = oof_cat\n",
    "    models['catboost'] = cat_model\n",
    "\n",
    "# --- Model 4: Gradient Boosting (sklearn) ---\n",
    "print(\"  Training GradientBoosting...\")\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "oof_gb = cross_val_predict(gb_model, X_train, y_train, cv=kf)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_mse = mean_squared_error(y_train, oof_gb)\n",
    "print(f\"    GradientBoosting OOF MSE: {gb_mse:.10f}\")\n",
    "oof_predictions['gb'] = oof_gb\n",
    "models['gb'] = gb_model\n",
    "\n",
    "# --- Model 5: Ridge Regression ---\n",
    "print(\"  Training Ridge...\")\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "oof_ridge = cross_val_predict(ridge_model, X_train_scaled, y_train, cv=kf)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "ridge_mse = mean_squared_error(y_train, oof_ridge)\n",
    "print(f\"    Ridge OOF MSE: {ridge_mse:.10f}\")\n",
    "oof_predictions['ridge'] = oof_ridge\n",
    "models['ridge'] = ridge_model\n",
    "\n",
    "# --- Model 6: MLP Neural Network ---\n",
    "print(\"  Training MLP...\")\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "oof_mlp = cross_val_predict(mlp_model, X_train_scaled, y_train, cv=kf)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "mlp_mse = mean_squared_error(y_train, oof_mlp)\n",
    "print(f\"    MLP OOF MSE: {mlp_mse:.10f}\")\n",
    "oof_predictions['mlp'] = oof_mlp\n",
    "models['mlp'] = mlp_model\n",
    "\n",
    "# --- Model 7: Extra Trees ---\n",
    "print(\"  Training ExtraTrees...\")\n",
    "et_model = ExtraTreesRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=12,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "oof_et = cross_val_predict(et_model, X_train, y_train, cv=kf)\n",
    "et_model.fit(X_train, y_train)\n",
    "et_mse = mean_squared_error(y_train, oof_et)\n",
    "print(f\"    ExtraTrees OOF MSE: {et_mse:.10f}\")\n",
    "oof_predictions['et'] = oof_et\n",
    "models['et'] = et_model\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: META-LEARNER STACKING\n",
    "# =============================================================================\n",
    "print(\"\\n[8] Training meta-learner for stacking...\")\n",
    "\n",
    "# Create stacking features from OOF predictions\n",
    "stack_train = np.column_stack([oof_predictions[k] for k in oof_predictions.keys()])\n",
    "print(f\"  Stacking features shape: {stack_train.shape}\")\n",
    "\n",
    "# Meta-learner: Ridge regression on OOF predictions\n",
    "meta_model = Ridge(alpha=0.5)\n",
    "oof_meta = cross_val_predict(meta_model, stack_train, y_train, cv=kf)\n",
    "meta_model.fit(stack_train, y_train)\n",
    "meta_mse = mean_squared_error(y_train, oof_meta)\n",
    "print(f\"  Meta-learner OOF MSE: {meta_mse:.10f}\")\n",
    "\n",
    "# Also try simple weighted average\n",
    "print(\"\\n  Finding optimal ensemble weights...\")\n",
    "best_ensemble_mse = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "# Grid search for weights\n",
    "model_names = list(oof_predictions.keys())\n",
    "n_models = len(model_names)\n",
    "\n",
    "# Simple averaging first\n",
    "avg_pred = np.mean([oof_predictions[k] for k in model_names], axis=0)\n",
    "avg_mse = mean_squared_error(y_train, avg_pred)\n",
    "print(f\"  Simple average MSE: {avg_mse:.10f}\")\n",
    "\n",
    "# Weighted by inverse MSE\n",
    "model_mses = {k: mean_squared_error(y_train, oof_predictions[k]) for k in model_names}\n",
    "inv_mses = {k: 1/(v + 1e-10) for k, v in model_mses.items()}\n",
    "total_inv = sum(inv_mses.values())\n",
    "opt_weights = {k: v/total_inv for k, v in inv_mses.items()}\n",
    "\n",
    "weighted_pred = np.zeros(len(y_train))\n",
    "for k, w in opt_weights.items():\n",
    "    weighted_pred += w * oof_predictions[k]\n",
    "weighted_mse = mean_squared_error(y_train, weighted_pred)\n",
    "print(f\"  Inverse-MSE weighted MSE: {weighted_mse:.10f}\")\n",
    "\n",
    "# Choose best\n",
    "if meta_mse < min(avg_mse, weighted_mse):\n",
    "    USE_META = True\n",
    "    final_train_mse = meta_mse\n",
    "    print(f\"\\n  ‚òÖ Using META-LEARNER (MSE={meta_mse:.10f})\")\n",
    "elif weighted_mse < avg_mse:\n",
    "    USE_META = False\n",
    "    final_train_mse = weighted_mse\n",
    "    print(f\"\\n  ‚òÖ Using WEIGHTED ENSEMBLE (MSE={weighted_mse:.10f})\")\n",
    "else:\n",
    "    USE_META = False\n",
    "    opt_weights = {k: 1/n_models for k in model_names}\n",
    "    final_train_mse = avg_mse\n",
    "    print(f\"\\n  ‚òÖ Using SIMPLE AVERAGE (MSE={avg_mse:.10f})\")\n",
    "\n",
    "print(f\"\\n  Model weights: {opt_weights}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 9: GENERATE TEST PREDICTIONS\n",
    "# =============================================================================\n",
    "print(\"\\n[9] Generating test predictions...\")\n",
    "\n",
    "test_lookup = {r['Profile_ID']: r for _, r in test_p.iterrows()}\n",
    "test_ids = sorted(test_p['Profile_ID'].unique().tolist())\n",
    "print(f\"  Test users: {len(test_ids)} | Pairs: {len(test_ids)**2}\")\n",
    "\n",
    "# Build test features\n",
    "results = []\n",
    "test_features = []\n",
    "\n",
    "for i, src in enumerate(test_ids):\n",
    "    if i % 100 == 0: print(f\"  Processing {i+1}/{len(test_ids)}...\")\n",
    "    src_row = test_lookup[src]\n",
    "    for dst in test_ids:\n",
    "        dst_row = test_lookup[dst]\n",
    "        \n",
    "        if src == dst:\n",
    "            computed = best_func(src_row, dst_row)\n",
    "            results.append({'ID': f\"{src}_{dst}\", 'is_self': True, 'formula': max(SELF_SCORE, computed)})\n",
    "        else:\n",
    "            test_features.append(extract_ultra_features(src_row, dst_row))\n",
    "            results.append({'ID': f\"{src}_{dst}\", 'is_self': False, 'formula': 0})\n",
    "\n",
    "print(\"  Computing model predictions...\")\n",
    "X_test = pd.DataFrame(test_features)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Get predictions from all models\n",
    "test_predictions = {}\n",
    "for name, model in models.items():\n",
    "    if name in ['ridge', 'mlp']:\n",
    "        test_predictions[name] = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        test_predictions[name] = model.predict(X_test)\n",
    "\n",
    "# Stack test predictions\n",
    "stack_test = np.column_stack([test_predictions[k] for k in oof_predictions.keys()])\n",
    "\n",
    "# Final prediction\n",
    "if USE_META:\n",
    "    final_test_preds = meta_model.predict(stack_test)\n",
    "else:\n",
    "    final_test_preds = np.zeros(len(X_test))\n",
    "    for k, w in opt_weights.items():\n",
    "        final_test_preds += w * test_predictions[k]\n",
    "\n",
    "# Clip predictions\n",
    "final_test_preds = np.clip(final_test_preds, 0, 1)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 10: CREATE SUBMISSION\n",
    "# =============================================================================\n",
    "print(\"\\n[10] Creating submission...\")\n",
    "\n",
    "sub_df = pd.DataFrame(results)\n",
    "non_self = ~sub_df['is_self']\n",
    "sub_df.loc[non_self, 'compatibility_score'] = final_test_preds.round(4)\n",
    "sub_df.loc[sub_df['is_self'], 'compatibility_score'] = sub_df.loc[sub_df['is_self'], 'formula'].round(4)\n",
    "\n",
    "sub_df = sub_df[['ID', 'compatibility_score']]\n",
    "\n",
    "# Validate\n",
    "assert len(sub_df) == len(test_ids)**2\n",
    "assert sub_df['compatibility_score'].between(0, 1).all()\n",
    "\n",
    "print(f\"  ‚úì Shape: {sub_df.shape}\")\n",
    "print(f\"  ‚úì Score range: [{sub_df['compatibility_score'].min():.4f}, {sub_df['compatibility_score'].max():.4f}]\")\n",
    "\n",
    "# Check self-pairs\n",
    "self_check = sub_df[sub_df['ID'].apply(lambda x: x.split('_')[0] == x.split('_')[1])]\n",
    "print(f\"  ‚úì Self-pairs: {len(self_check)}, score={self_check['compatibility_score'].iloc[0]}\")\n",
    "\n",
    "print(\"\\n  Sample:\")\n",
    "print(sub_df.head(10).to_string(index=False))\n",
    "\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "print(\"\\n  ‚úì Saved: submission.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# DONE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üî• ULTRA CHAMPIONSHIP SOLUTION COMPLETE! üî•\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  ULTRA WINNING CONFIGURATION                                         ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  ‚Ä¢ Models: {len(models)} (LGB/XGB/CAT/GB/Ridge/MLP/ET)                      ‚ïë\n",
    "‚ïë  ‚Ä¢ Features: {X_train.shape[1]:<47} ‚ïë\n",
    "‚ïë  ‚Ä¢ Stacking: {'META-LEARNER' if USE_META else 'WEIGHTED ENSEMBLE':<40} ‚ïë\n",
    "‚ïë  ‚Ä¢ Final Train MSE: {final_train_mse:<40.10f} ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "\"\"\")\n",
    "for name, mse in model_mses.items():\n",
    "    print(f\"   ‚Ä¢ {name:<12}: MSE={mse:.10f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ENSEMBLE WEIGHTS:\n",
    "\"\"\")\n",
    "for name, w in opt_weights.items():\n",
    "    print(f\"   ‚Ä¢ {name:<12}: {w:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "OUTPUT: submission.csv ({len(sub_df):,} rows)\n",
    "\n",
    "üèÜ Good luck! This is maximum optimization! üèÜ\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15545167,
     "sourceId": 129596,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6885.70694,
   "end_time": "2026-02-02T13:50:47.339876",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-02T11:56:01.632936",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
